#!/usr/bin/env python
"""
PyConform - Command-Line Interface

This command-line tool is designed to run PyConform, given a "standardization file" (a
JSON-formatted file specifying the output variables, files and formats, and how to construct
the output variables from input variables) and a collection of "input NetCDF files" containing
the data to be standardized.

COPYRIGHT: 2017, University Corporation for Atmospheric Research
LICENSE: See the LICENSE.rst file for details
"""

from os.path import exists
from glob import glob
from json import load as json_load
from collections import OrderedDict
from argparse import ArgumentParser, ArgumentTypeError
from warnings import simplefilter
from datetime import datetime
from imp import load_source

from asaptools.simplecomm import create_comm
from asaptools.partition import Duplicate

from pyconform.datasets import InputDatasetDesc, OutputDatasetDesc
from pyconform.dataflow import DataFlow
from pyconform.flownodes import ValidationWarning


#=========================================================================
# chunk - Chunksize for a named dimension
#=========================================================================
def chunk(arg):
    try:
        name, size_str = arg.split(',')
        size = int(size_str)
        return name, size
    except:
        raise ArgumentTypeError("Chunks must be formatted as 'name,size'")


#=========================================================================
# Command-line Interface
#=========================================================================
def cli(argv=None):
    desc = """This is the PyConform command-line tool.  This scripts takes
              input from the command-line and a predefined output
              specification file (specfile)."""

    parser = ArgumentParser(description=desc)
    parser.add_argument('-c', '--chunk', dest='chunks', default=[],
                        metavar='NAME,SIZE', action='append', type=chunk,
                        help=('Chunk sizes for each dimension specified in the '
                              'output specification file.  Data will be read/written '
                              'in sizes given by these chunks. [Default: no chunking]'))
    parser.add_argument('-d', '--deflate', default=None, metavar='DEFLATELEVEL', type=int,
                        help=('Override deflate levels of all output files with given value.  '
                              '(can be any integer from 0 to 9, with 0 meaning no compression)'))
    parser.add_argument('--debug', default=False, action='store_true',
                        help=('Whether to enable rudimentary debug features'))
    parser.add_argument('-e', '--error', default=False, action='store_true',
                        help=('Whether to error when validation checks do not pass (True) or '
                              'simply print a warning message (False) [Default: False]'))
    parser.add_argument('-f', '--stdfile', default=None, metavar='STANDARDIZATION', type=str,
                        help=('JSON-formatted standardization (output specification) file '
                              '[REQUIRED]'))
    parser.add_argument('-m', '--module', default=None, metavar='MODULE', action='append',
                        help=('Module file path with user-defined functions that must be loaded '
                              'before the Conformation operation can be done'))
    parser.add_argument('-n', '--no_history', default=False, action='store_true',
                        help=('Whether to omit the addition of the "history" attribute in each '
                              'output variable, which stores the provenance information generated '
                              'at execution time [Default: False]'))
    parser.add_argument('-s', '--serial', default=False, action='store_true',
                        help=('Whether to run in serial (True) or parallel '
                              '(False). [Default: False]'))
    parser.add_argument('infiles', metavar='INFILE', nargs='*', type=str,
                        help=('Input file path or globstring specifying input data for the '
                              'PyConform operation.  If no input files are specified, then '
                              'PyConform will validate the output specification file only, and '
                              'then exit.  [No default]'))

    return parser.parse_args(argv)


#=========================================================================
# Main Script Function
#=========================================================================
def main(argv=None):
    args = cli(argv)

    # Create the necessary SimpleComm
    scomm = create_comm(serial=args.serial)

    # Do setup only on manager node
    if scomm.is_manager():

        # Check that the specfile exists
        if not exists(args.stdfile):
            raise OSError(('Output specification file {!r} not '
                           'found').format(args.stdfile))

        # Read the specfile into a dictionary
        print 'Reading standardization file: {}'.format(args.stdfile)
        dsdict = json_load(open(args.stdfile, 'r'),
                           object_pairs_hook=OrderedDict)

        # Parse the output Dataset
        print 'Creating output dataset descriptor from standardization file...'
        outds = OutputDatasetDesc(dsdict=dsdict)

    else:
        outds = None

    # Send the output descriptor to all nodes
    outds = scomm.partition(outds, func=Duplicate(), involved=True)

    # Sync
    scomm.sync()

    # Continue setup only on manager node
    if scomm.is_manager():

        # Gather the list of input files
        infiles = []
        for infile in args.infiles:
            infiles.extend(glob(infile))

        # If no input files, stop here
        if len(infiles) == 0:
            print 'Standardization file validated.'
            return

        # Parse the input Dataset
        print 'Creating input dataset descriptor from {} input files...'.format(len(infiles))
        inpds = InputDatasetDesc(filenames=infiles)

    else:
        inpds = None

    # Send the input descriptor to all nodes
    inpds = scomm.partition(inpds, func=Duplicate(), involved=True)

    # Sync and continue process on all nodes
    scomm.sync()

    # Check for warn/error
    if args.error:
        simplefilter("error", ValidationWarning)

    # Try importing all of the necessary user-defined modules
    if args.module is not None:
        for i, modpath in enumerate(args.module):
            load_source('user{}'.format(i), modpath)

    # Setup the PyConform data flow on all nodes
    if scomm.is_manager():
        print 'Creating the data flow...'.format(len(infiles))
    dataflow = DataFlow(inpds, outds)

    # Execute the data flow (write to files)
    history = not args.no_history
    dataflow.execute(chunks=dict(args.chunks), scomm=scomm, history=history,
                     deflate=args.deflate, debug=args.debug)


#=========================================================================
# Command-line Operation
#=========================================================================
if __name__ == '__main__':
    main()
